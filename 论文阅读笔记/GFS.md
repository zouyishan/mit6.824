# 设计概述
## 接口
类似传统文件系统的API接口函数，不是严格按照POSIX等标准的API形式
还是支持创建文件，删除文件，打开文件，关闭文件，读和写文件

GFS提供了快照和追加操作。
* 快照：快照是以很低的成本创建一个文件或者目录树的拷贝。
* 追加：追加操作允许多个客户端同时对一个文件进行数据追加操作，同时保证每个客户端的追加操作都是原子性的。多个客户端可以在不需要额外的同步锁定的情况下，同时对一个文件追加数据。

## 架构
一个逻辑Master节点(注意是逻辑，可以是多个物理节点)，多台chunk服务器，并且同时被多个客户端访问。可以把chunk服务器和客户端都放到一个物理机上，前提是要机器资源允许(核多内存多)
![GFS架构图](https://user-images.githubusercontent.com/57765968/168441111-7ad18ee5-83fd-4f1c-ae46-29bee0ba2662.png)

GFS存储的文件都被分为大小相同的chunk，master会分配个每个chunk不变的，全球唯一的64位标识。可以通过制定的chunk标识和字节范围来读写数据。

**Master的作用**
1. Master节点管理所有文件系统的源信息，包括：文件和chunk的命名空间，文件和chunk的映射信息，当前chunk的位置信息，访问控制信息。
2. 管理着系统范围内的活动，如 Chunk租用管理，孤儿Chunk的回收，Chunk在服务器之间的迁移。
3. Master节点使用心跳信息周期地和每个**Chunk服务器**通讯，发送指令给Chunk服务器或者接受Chunk服务器的状态

GFS客户端代码以库的形式被链接到客户程序里，客户端代码实现了API接口和Master，Chunk通信。客户端和Master交流不多，大多是和Chunk服务器进行交互。由于不提供POSIX标准的API功能，所以不需要深入到Linux code级别。

同时无论是客户端还是Chunk服务器都不会缓存文件数据。
**客户端不会缓存数据**：
- 大部分数据都是由流来读取
- 由于读取的数据较大，工作集太大无法缓存。
- 简化了客户端和整个系统的设计和实现
**Chunk不会缓存数据**：
- 服务器自带的Linux文件系统会将热点数据缓存到内存中。

## 单一Master节点(逻辑单节点)

客户端不会从Master节点读写文件数据，所以Master节点不会成为性能瓶颈。

客户端将从Master读取的元数据信息缓存一段时间，后续的操作将直接和Chunk服务器进行数据读写操作。

- 客户端把文件名和程序指定的字节偏移，根据固定的Chunk大小，转换成文件的Chunk索引
- 把文件名和Chunk索引发送给Master节点。
- Master节点将对应Chunk的标识和位置信息(以及副本的位置信息)发送给客户端
- 客户端用文件名和Chunk索引作为key缓存这些信息

客户端会选择最近的副本进行通信。在后续就没有必要和Master节点进行通信了。除非缓存信息失效，或者又需要打开文件。

客户端通常一个请求中查询多个Chunk的信息，Master节点的回应也可能包含紧跟着请求Chunk后面Chunk的信息，这样就避免了未来可能发生的通信。

## Chunk的大小
每个Chunk的大小是64MB，惰性空间分配策略避免了因内部碎片造成的空间浪费，内部碎片或许是对选择这么大的Chunk尺寸最有争议的一点。

> 问：惰性空间分配如何避免内部碎片造成的浪费？

大Chunk的优点：
- Master存的元信息变少
- Client和Chunk的交互也会变少，同时Client能缓存更多的Chunk信息
- Client可以在大的Chunk上执行更多的操作，可以用长连接避免网络开销
大Chunk的缺点：
- 比较小的影响是热点文件，因为程序通常是连续的读取包含多个Chunk的大文件


但是热点问题还是在执行批处理的时候发生了，解决方案是：更大的复制参数来保存可执行文件，以及错开批处理队列系统启动时间的方法来解决这个问题。

更加长效的解决方案是，在这种可执行文件被多点读取的时候允许客户端从其他客户端读取数据。

## 元数据
这里的Master节点都是物理的Master节点，不是逻辑上的Master节点。

存储主要有三种元数据：
- 文件和Chunk的命名空间
- 文件和Chunk的对应关系
- 每个Chunk的副本的存放地点


所有元信息都保存在Master服务器的内存中。前两种信息(命名空间，文件和Chunk的对应关系)会以记录变更日志的方式记录在内存操作系统的系统日志文件中，同时这些日志也会复制到其他Master机器上。
采用日志的方式就不用担心丢失Master的状态，并且不用担心Master数据不一致的问题。

Master 不持久的记录 chunk 的信息，而是在启动后以轮询的方式获取这些信息，并通过心跳保护状态最新。因为只有 **chunk server 对 chunk 才有最终决定权**。

### 内存中的数据结构
对于64MB的文件数据信息，我们只需要64字节不到的内存来记录 命名空间和文件映射。所以内存不是瓶颈。如果文件增加的较多，Master增加的内存也是很少的。

### Chunk位置信息
Master服务器只是在启动的时候轮询Chunk服 务器以获取这些信息。Master服务器能够保证它持有的信息始终是最新的，因为它控制了所有的Chunk位置的分配，而且通过周期性的心跳信息监控 Chunk服务器的状态。

- 简化了Chunk服务器加入集群，离开集群，更名，失效，以及重启的时候，Master服务器和Chunk服务器数据同步的问题。
- 另外，只有Chunk服务器才能确定一个Chunk是否在硬盘上。因为Chunk服务器的错误可能会导致Chunk自动消失(硬盘坏了无法访问)

### 操作日志
操作日志包含元数据变更信息，对GFS非常重要！
1. 元数据唯一的持久化存储记录
2. 作为判断同步操作顺序的逻辑时间线

必须保证日志文件的完整性，确保元数据的变化被持久化后，日志才对客户端是可见的。

要是不保证持久化，即使Chunk没有问题，也可能丢失掉整个文件系统。所以只有把元数据写到本地磁盘或者机器硬盘后，才会响应客户端的操作请求。同时Master会收集多个日志记录后批量操作，可以减少磁盘和复制对系统整体性能的影响。

同时为了缩短Master启动时间，操作日志必须足够小。

Master服务器在日志增长到一定量时，将所有的状态数据写入一个Checkpoint文件，灾难恢复时只有读取这个checkpoint文件，然后执行后面少量的日志即可。Checkpoint文件以压缩B-树形势的数据结构存储，可以直接映射到内存，在用于命名空间查询时无需额外的解析。

对于一个包含数百万个文件的集群，创建一个Checkpoint文件需要1分钟左右的时间。**创建完成后，Checkpoint 文件会被写入在本地和远程的硬盘里**。

Master服务器恢复只需要最新的Checkpoint文件和后续的日志文件。旧的Checkpoint文件和日志文件可以被删除，但是为了应对灾难性的故障，我们通常会多保存一些历史文件。Checkpoint失败不会对正确性产生任何影响，因为恢复功能的代码可以检测并跳过没有完成的Checkpoint文件。

